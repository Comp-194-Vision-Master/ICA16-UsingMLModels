"""
File: mediapipeFaceDetect.py
Date: Fall 2025

This program provides a demo showing how to use Mediapipe's simple face detection model, and to visualize the results.
"""

import math
import cv2

import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

MARGIN = 10  # pixels
ROW_SIZE = 10  # pixels
FONT_SIZE = 1
FONT_THICKNESS = 1
CIRCLE_COLOR = (0, 255, 0)  # green
TEXT_COLOR = (0, 255, 255)  # cyan, remembering that this is applied to an RGB, not a BGR, image


def runFaceDetect(source=0):
    """Main program, sets up the blaze face detection model and then runs it on a video feed."""

    # Set up model
    modelPath = "MediapipeModels/blaze_face_short_range.tflite"
    base_options = python.BaseOptions(model_asset_path=modelPath)
    options = vision.FaceDetectorOptions(base_options=base_options)
    detector = vision.FaceDetector.create_from_options(options)

    # Set up camera
    cap = cv2.VideoCapture(source)

    i = 0
    while True:
        gotIm, frame = cap.read()
        if not gotIm:
            break

        # Convert the frame to a Mediapipe image representation
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)

        # Run the face detector model on the image
        detect_result = detector.detect(mp_image)

        # TODO: Uncomment this call to run the function that checks which way each detected face is pointing
        # findFacing(detect_result)

        annot_image = visualizeResults(mp_image.numpy_view(), detect_result)

        # Display the results on screen
        vis_image = cv2.cvtColor(annot_image, cv2.COLOR_RGB2BGR)
        cv2.imshow("Detected", vis_image)

        x = cv2.waitKey(10)
        if x > 0:
            if chr(x) == 'q':
                break
            elif chr(x) == 's':
                cv2.imwrite("faceDetect" + str(i) + ".png", vis_image)
                i += 1
    cap.release()


def _normalized_to_pixel_coordinates(normalized_x, normalized_y, image_width, image_height):
    """Converts normalized value pair to pixel coordinates."""

    # Checks if the float value is between 0 and 1.
    def is_valid_normalized_value(value):
        return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))

    if not is_valid_normalized_value(normalized_x):
        normalized_x = max(0.0, min(1.0, normalized_x))
    if not is_valid_normalized_value(normalized_y):
        normalized_y = max(0.0, min(1.0, normalized_y))
    x_px = min(math.floor(normalized_x * image_width), image_width - 1)
    y_px = min(math.floor(normalized_y * image_height), image_height - 1)
    return x_px, y_px


def visualizeResults(image, detection_result):
    """Draws bounding boxes and keypoints on the input image and return it.
    Args:
        image: The input RGB image.
        detection_result: The list of all "Detection" entities to be visualized.
    Returns: Image with bounding boxes.
    """

    # Copy the original image and make changes to the copy
    annotated_image = image.copy()
    height, width, _ = image.shape

    for detection in detection_result.detections:
        # Draw bounding_box for each face detected
        bbox = detection.bounding_box
        start_point = bbox.origin_x, bbox.origin_y
        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height
        cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)

        # Draw face keypoints for each face detected
        for keypoint in detection.keypoints:
            keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y, width, height)
            cv2.circle(annotated_image, keypoint_px, 3, CIRCLE_COLOR, -1)

        # Draw category label and confidence score as text on bounding box
        category = detection.categories[0]
        category_name = category.category_name
        category_name = '' if category_name is None else category_name
        probability = round(category.score, 2)
        result_text = category_name + ' (' + str(probability) + ')'
        text_location = (MARGIN + bbox.origin_x,
                         MARGIN + ROW_SIZE + bbox.origin_y)
        cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,
                    FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)

    return annotated_image


def findFacing(detect_results):
    """Takes in the face detection results and determines, for each face located, whether the
    face is pointing forward, to the left, or to the right. It prints a message with the results."""
    # TODO: for each face detected, determine the facing from the relative positions of each eye and
    # TODO: the edge of the face on that side
    pass


if __name__ == "__main__":
    runFaceDetect(0)
